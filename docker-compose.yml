version: '3.8'

services:
  ollama:
    image: ollama/ollama:latest  # Official Ollama image
    container_name: ollama
    ports:
      - "11434:11434"  # Exposing Ollama API port
    volumes:
      - ollama_data:/root/.ollama  # Persist model data if needed

  fastapi:
    build:
      context: .              # Set build context to the repository root
      dockerfile: api/Dockerfile  # Specify the Dockerfile location relative to the context
    container_name: fastapi
    ports:
      - "8000:8000"  # Expose FastAPI app on port 8000
    depends_on:
      - ollama  # Ensure Ollama is running before starting FastAPI
    environment:
      - OLLAMA_API_URL=http://ollama:11434/api/generate  # Use Ollama service's internal Docker network address
    volumes:
      - ./api:/api  # Mount the 'api/' directory into the container at '/api'
      - ./wait-for-it.sh:/wait-for-it.sh  # Ensure 'wait-for-it.sh' is accessible
    entrypoint:
      - /bin/sh
      - -c
      - |
        chmod +x /wait-for-it.sh && /wait-for-it.sh ollama:11434 -- uvicorn api.api:app --host 0.0.0.0 --port 8000

volumes:
  ollama_data:
