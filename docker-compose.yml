version: '3.8'

services:
  ollama:
    image: ollama/ollama:latest  # Official Ollama image
    container_name: ollama
    ports:
      - "11434:11434"  # Exposing Ollama API port
    volumes:
      - ollama_data:/root/.ollama  # Persist model data if needed

  fastapi:
    build:
      context: ./api  # Build FastAPI image from the Dockerfile inside 'api' folder
    container_name: fastapi
    ports:
      - "8000:8000"  # Expose FastAPI app on port 8000
    depends_on:
      - ollama  # Ensure Ollama is running before starting FastAPI
    environment:
      - OLLAMA_API_URL=http://ollama:11434/api/generate  # Use Ollama service's internal Docker network address
    volumes:
      - ./api:/api  # Only mount 'api/' directory into the container
      - ./wait-for-it.sh:/wait-for-it.sh  # Ensure 'wait-for-it.sh' is accessible
    entrypoint:
      - /bin/sh
      - -c
      - |
        chmod +x /wait-for-it.sh
        /wait-for-it.sh ollama:11434 -- uvicorn api.api:app --host 0.0.0.0 --port 8000
    # Optional: expose any additional ports that your FastAPI app might need internally
    expose:
      - "5005"  # Optional, can be removed if not needed

volumes:
  ollama_data:  # Persistent volume for Ollama models and data
